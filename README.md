# Latent Zoom Trajectories and Semantic Direction Discovery in Stable Diffusion

This project investigates semantic transformations in the latent space of **Stable Diffusion XL (SDXL)**, beginning with the zoom-in effect. The goal is to treat visual manipulations such as "zoom" as **vector directions** in latent space - similar to how word embeddings like Word2Vec express semantic relationships (for example, `king - man + woman ≈ queen` [1]).

## Objectives

- Encode a base image and a semantically zoomed version using SDXL’s VAE.
- Extract a latent vector delta representing the transformation from base to zoomed image.
- Apply this transformation to other images in latent space and decode them to test semantic transfer.
- Extend this technique to additional conceptual transformations, including:
  - Spatial changes (for example, rotation, translation)
  - Style or abstraction shifts (for example, realism to surrealism)
  - Affective tone (for example, sadness to joy)
  - Temporal or biological transitions (for example, age progression)

This approach builds upon prior work in latent space arithmetic [2][3] and directional editing using CLIP or diffusion models [4][5], aiming to develop modular and interpretable control over generative models.

## References

1. Mikolov et al. (2013). Efficient Estimation of Word Representations in Vector Space. https://arxiv.org/abs/1301.3781  
2. Radford et al. (2015). Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. https://arxiv.org/abs/1511.06434  
3. White (2016). Sampling Generative Networks: Notes on a few effective techniques. https://arxiv.org/abs/1609.04468  
4. Shen et al. (2020). Interpreting the Latent Space of GANs for Semantic Face Editing. https://arxiv.org/abs/1907.10786  
5. Patashnik et al. (2021). StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery. https://arxiv.org/abs/2103.17249  
6. Voynov and Babenko (2020). Unsupervised Discovery of Interpretable Directions in the GAN Latent Space. https://arxiv.org/abs/2002.03754  

## Progress to Date

### Image Preparation

- Selected a high-resolution 1024×1024 image generated by SDXL.
- Resized it to 512×512 to ensure compatibility with the VAE encoder.

### Encoding to Latent Space

- Normalized the image to the [-1, 1] range.
- Encoded using SDXL’s VAE to obtain a latent tensor of shape [1, 4, 64, 64].

### Zoom Simulation in Latent Space

- Simulated a zoom-in operation by cropping central regions of the latent tensor and resizing them back to [1, 4, 64, 64] using bilinear interpolation.
- This approximates the perceptual effect of zooming without re-generating images through the full diffusion model.

### Decoding and Output

- Decoded the modified latent tensors back to RGB space using the VAE decoder.
- The results showed a clear zoom effect, demonstrating that perceptual zooming can be achieved using only the VAE.

### Validation and Observations

- Visual continuity and semantic directionality were preserved.
- Some desaturation and blurring were observed, consistent with bypassing the full denoising pipeline.

## Next Steps

### 1. Transferable Semantic Zoom Vectors

**Goal**: Extract a reusable latent-space "zoom" vector (delta)  
**Steps**:
- Subtract the base and final latent tensors to compute Δz
- Normalize and apply it incrementally to new latent encodings
- Decode and evaluate semantic consistency

### 2. Align Pixel and Latent Zoom Sequences

**Goal**: Compare PIL-based zoom sequences with latent-space crops  
**Steps**:
- Generate a zoom sequence in pixel space (for example, using PIL)
- Encode each frame with SDXL’s VAE
- Compare these encodings to interpolated latent zoom tensors
- Optionally train a small model to learn this mapping

### 3. Identify Additional Semantic Directions

Explore other interpretable transformations such as:
- Pan (spatial shift)
- Age (young to old)
- Lighting (day to night)
- Emotional tone (neutral to expressive)

These can be approached via:
- Prompt-difference encoding
- Interpolation between latent exemplars
- Latent deltas derived from paired transformations

## Immediate Next Steps

1. **Extract Latent Delta**
   - Compute Δz = latent_zoomed - latent_base
   - Normalize or scale the vector
   - Save for reuse

2. **Create Test Script**
   - Encode a different source image
   - Apply scaled increments of Δz
   - Decode and inspect the results

3. **Evaluate and Refine**
   - Assess semantic consistency
   - Adjust scaling, apply smoothing or PCA if necessary

## Contribution

This project aims to develop interpretable, composable controls in generative image modeling. By manipulating latent space vectors directly, it complements or improves upon traditional prompt engineering and model fine-tuning approaches.
